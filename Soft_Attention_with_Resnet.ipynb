{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTATION OF SOFT_ATTENTION+RESNET50\n",
        "The following cells in the notebook aligns with the baseline code provided in the Algonauts challenge. GoogleColab Pro or Jupyter notebook is the preferred platform for execution of this program due to computational complexities and memory overflow issues. \n",
        "Different versions of this pipeline with variants of resnet and ensemble model were executed on MacBook Pro with batch processing and modular programming of sequential jobs.\n",
        "\n",
        "NOTE: this notebook is just for comprehending the pipeline and might encounter with overflow issues if executed\n",
        "\n",
        "For more details, kindly contact : ngurupra@ualberta.ca"
      ],
      "metadata": {
        "id": "nGmCkrw3DdYZ"
      },
      "id": "nGmCkrw3DdYZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64913f3b",
      "metadata": {
        "id": "64913f3b"
      },
      "outputs": [],
      "source": [
        "!pip install nilearn\n",
        "!pip install decord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db624250",
      "metadata": {
        "id": "db624250"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import urllib\n",
        "import torch\n",
        "import cv2\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "import pickle\n",
        "from nilearn import plotting\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms as trn\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.autograd import Variable as V\n",
        "from sklearn.decomposition import PCA, IncrementalPCA\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "from nilearn import datasets\n",
        "from nilearn import surface\n",
        "from decord import VideoReader\n",
        "from decord import cpu\n",
        "import torch.optim as optim\n",
        "from torch.nn import init\n",
        "import functools\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11948cbe",
      "metadata": {
        "id": "11948cbe"
      },
      "outputs": [],
      "source": [
        "download_link = 'https://www.dropbox.com/s/agxyxntrbwko7t1/participants_data.zip?dl=0' \n",
        "os.environ[\"download_link\"] = download_link\n",
        "!echo $download_link \n",
        "!wget -O participants_data.zip -c $download_link  \n",
        "!unzip participants_data.zip\n",
        "!wget -O example.nii -c https://github.com/Neural-Dynamics-of-Visual-Cognition-FUB/Algonauts2021_devkit/raw/main/example.nii\n",
        "!wget -c https://raw.githubusercontent.com/Neural-Dynamics-of-Visual-Cognition-FUB/Algonauts2021_devkit/main/class_names_ImageNet.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f907d88",
      "metadata": {
        "id": "3f907d88"
      },
      "outputs": [],
      "source": [
        "def save_dict(di_, filename_):\n",
        "    with open(filename_, 'wb') as f:\n",
        "        pickle.dump(di_, f)\n",
        "\n",
        "def load_dict(filename_):\n",
        "    with open(filename_, 'rb') as f:\n",
        "        u = pickle._Unpickler(f)\n",
        "        u.encoding = 'latin1'\n",
        "        ret_di = u.load()\n",
        "    return ret_di\n",
        "\n",
        "def visualize_activity(vid_id,sub):\n",
        "  fmri_dir = '/Users/nam/Desktop/participants_data/participants_data_v2021'\n",
        "  track = \"full_track\"\n",
        "  results_dir = '/Users/nam/Desktop/'\n",
        "  track_dir = os.path.join(fmri_dir, track) \n",
        "  sub_fmri_dir = os.path.join(track_dir, sub)\n",
        "  fmri_train_all,voxel_mask = get_fmri(sub_fmri_dir,\"WB\") \n",
        "  visual_mask_3D = np.zeros((78,93,71))\n",
        "  visual_mask_3D[voxel_mask==1]= fmri_train_all[vid_id,:]\n",
        "  brain_mask = '/Users/nam/Desktop/example.nii'\n",
        "  nii_save_path =  os.path.join(results_dir, 'vid_activity.nii')\n",
        "  saveasnii(brain_mask,nii_save_path,visual_mask_3D)\n",
        "  plotting.plot_glass_brain(nii_save_path,\n",
        "                          title='fMRI response',plot_abs=False,\n",
        "                          display_mode='lyr',colorbar=True)\n",
        "\n",
        "def get_activations(activations_dir, layer_name):\n",
        "    #loads neural network features/activations into a numpy array \n",
        "    train_file = os.path.join(activations_dir,\"train_\" + layer_name + \".npy\")\n",
        "    test_file = os.path.join(activations_dir,\"test_\" + layer_name + \".npy\")\n",
        "    print(\"train_file\", train_file)\n",
        "    print(\"test_file\", test_file)\n",
        "    train_activations = np.load(train_file)\n",
        "    test_activations = np.load(test_file)\n",
        "    scaler = StandardScaler()\n",
        "    train_activations = scaler.fit_transform(train_activations)\n",
        "    test_activations = scaler.fit_transform(test_activations)\n",
        "    print(\"Tr A\", train_activations)\n",
        "    print(\"Te A\", test_activations)\n",
        "    return train_activations, test_activations\n",
        "\n",
        "def get_fmri(fmri_dir, ROI):\n",
        "    #loads fMRI data into a numpy array for an ROI\n",
        "    #Loading ROI \n",
        "    ROI_file = os.path.join(fmri_dir, ROI + \".pkl\")\n",
        "    ROI_data = load_dict(ROI_file)\n",
        "    #averaging ROI data across repetitions\n",
        "    ROI_data_train = np.mean(ROI_data[\"train\"], axis = 1)\n",
        "    if ROI == \"WB\":\n",
        "        voxel_mask = ROI_data['voxel_mask']\n",
        "        return ROI_data_train, voxel_mask\n",
        "    return ROI_data_train\n",
        "\n",
        "def saveasnii(brain_mask,nii_save_path,nii_data):\n",
        "    img = nib.load(brain_mask)\n",
        "    nii_img = nib.Nifti1Image(nii_data, img.affine, img.header)\n",
        "    nib.save(nii_img, nii_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf030f45",
      "metadata": {
        "id": "bf030f45"
      },
      "outputs": [],
      "source": [
        "#Loading fMRI data and inspecting dimensions\n",
        "sub = 'sub06'  #@param [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]  \n",
        "ROI = 'WB'  #@param [\"WB\", \"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n",
        "fmri_dir = '/Users/nam/Desktop/participants_data/participants_data_v2021' \n",
        "if ROI == \"WB\":\n",
        "    track = \"full_track\"\n",
        "else:\n",
        "    track = \"mini_track\"\n",
        "results_dir = '/Users/nam/Desktop/'\n",
        "track_dir = os.path.join(fmri_dir, track) \n",
        "sub_fmri_dir = os.path.join(track_dir, sub)\n",
        "if track == \"full_track\":\n",
        "    fmri_train_all,voxel_mask = get_fmri(sub_fmri_dir,ROI)\n",
        "else:\n",
        "    fmri_train_all = get_fmri(sub_fmri_dir,ROI)\n",
        "f, ax = plt.subplots(figsize=(12,5))\n",
        "ax.set(xlabel=\"Voxel\", ylabel=\"Stimulus\")\n",
        "heatmap = ax.imshow(fmri_train_all, aspect=\"auto\",cmap='jet',vmin=-1,vmax=1)\n",
        "f.colorbar(heatmap, shrink=.5, label=\"Response amplitude (Z)\")\n",
        "f.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dae9da0",
      "metadata": {
        "id": "9dae9da0"
      },
      "outputs": [],
      "source": [
        "#Visualize video\n",
        "vid_id = 270 #@param {type: \"integer\"}\n",
        "video_dir = '/Users/nam/Desktop/participants_data/AlgonautsVideos268_All_30fpsmax'\n",
        "video_list = glob.glob(video_dir + '/*.mp4')\n",
        "video_list.sort()\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(video_list[vid_id],'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "    <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f956349",
      "metadata": {
        "id": "0f956349"
      },
      "outputs": [],
      "source": [
        "#Visualize corresponding brain response\n",
        "visualize_activity(vid_id,sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cb1d36e",
      "metadata": {
        "id": "5cb1d36e"
      },
      "outputs": [],
      "source": [
        "class sa_layer(nn.Module):\n",
        "    #Constructs a module for attention: \n",
        "    #channel: Number of channels of the input feature map\n",
        "    #k_size: Adaptive selection of kernel size\n",
        "    def __init__(self, channel, k_size=3):\n",
        "        super(sa_layer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # feature descriptor on the global spatial information\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
        "        y = self.sigmoid(y)\n",
        "        return x * y.expand_as(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f33341d",
      "metadata": {
        "id": "6f33341d"
      },
      "outputs": [],
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,padding=1, bias=False)\n",
        "\n",
        "class SABasBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n",
        "        super(SABasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.sa = sa_layer(planes, k_size)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.sa(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SABottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, k_size=3):\n",
        "        super(SABottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sa = sa_layer(planes * 4, k_size)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.sa(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000, k_size=[3, 3, 3, 3]):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], int(k_size[0]))\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], int(k_size[1]), stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], int(k_size[2]), stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], int(k_size[3]), stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)        \n",
        "        self.fc0 = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 4 * 2, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.fc1 =nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, num_classes),\n",
        "            )\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, k_size, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, k_size))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, k_size=k_size))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)       \n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        out4 = self.layer4(out3)\n",
        "        xx = self.avgpool(out4)\n",
        "        xx = xx.view(xx.size(0), -1)    \n",
        "        out5 = self.fc0(xx)\n",
        "        out6= self.fc1(out5)\n",
        "        out7 = self.fc2(out6)\n",
        "        return x,out1, out2, out3,out4,out5, out6,out7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ce8ef6",
      "metadata": {
        "id": "c1ce8ef6"
      },
      "outputs": [],
      "source": [
        "def sa_resnet50(k_size=[3, 3, 3, 3], num_classes=1000, pretrained=False):\n",
        "    #Constructs a ResNet-50 model.\n",
        "    print(\"Constructing sa_resnet50......\")\n",
        "    model = ResNet(SABottleneck, [3, 4, 6, 3], num_classes=num_classes, k_size=k_size)\n",
        "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cfdd3b5",
      "metadata": {
        "id": "2cfdd3b5"
      },
      "outputs": [],
      "source": [
        "model2 = sa_resnet50()\n",
        "x = torch.randn(2,3,224,224)\n",
        "y = model2(x).to('cuda')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4327721d",
      "metadata": {
        "id": "4327721d"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "def nettttt():\n",
        "    net = models.resnet50(pretrained=True) \n",
        "    #pytorch has pre-defined model structure that can be directly loaded\n",
        "    net.fc = nn.Sequential(\n",
        "            nn.Sequential(\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(256 * 4 * 2, 1024),\n",
        "                nn.ReLU(inplace=True),\n",
        "                ),\n",
        "                nn.Sequential(\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(1024, 1024),\n",
        "                nn.ReLU(inplace=True),\n",
        "                ),\n",
        "                nn.Sequential(\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(1024, 1000)\n",
        "                ))\n",
        "    for param_tensor in net.state_dict():\n",
        "        print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3928513a",
      "metadata": {
        "id": "3928513a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "def load_resnet(checkpoint_path):\n",
        "    #weights intialization\n",
        "    model1 = nettttt()\n",
        "    model2 = sa_resnet50() \n",
        "    model_file = '/Users/nam/Desktop/resnet.pth'\n",
        "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "    model_dict =[\"conv1.weight\", \"bn1.bias\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.bias\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.bias\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.bias\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.bias\", \"fc0.1.weight\", \"fc0.1.bias\", \"fc1.1.weight\", \"fc1.1.bias\", \"fc2.1.weight\", \"fc2.1.bias\"]    #model_dict = [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\", \"fc1\", \"fc2\", \"fc3\"]\n",
        "    state_dict={}\n",
        "    i=0\n",
        "    for param_tensor in model1.state_dict():\n",
        "        if model_dict[i] == param_tensor:\n",
        "            state_dict[model_dict[i]] =  model1.state_dict()[param_tensor]\n",
        "            i = i+1\n",
        "    \n",
        "    model2.load_state_dict(state_dict, strict=False)\n",
        "    if torch.cuda.is_available():\n",
        "        model2.cuda()\n",
        "    for i in range(50): #less epochs due to computation complexity\n",
        "      optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.9)\n",
        "      model2 = model2.train()\n",
        "    return model2\n",
        "\n",
        "\n",
        "def print_resnet_predictions(output):\n",
        "    with open('class_names_ImageNet.txt') as labels:\n",
        "        categories = [s.strip() for s in labels.readlines()]\n",
        "\n",
        "    # sort the probability vector in descending order\n",
        "    sorted, indices = torch.sort(output, descending=True)\n",
        "    percentage = F.softmax(output, dim=1)[0] * 10000.0\n",
        "    top5_prob, top5_catid = torch.topk(percentage, 5)\n",
        "    for i in range(top5_prob.size(0)):\n",
        "        print(categories[top5_catid[i]], top5_prob[i].item())\n",
        "\n",
        "    results = [(categories[i], percentage[i].item()) for i in indices[0][:5]]\n",
        "    for i in range(5):\n",
        "        print('{}: {:.4f}%'.format(results[i][0], results[i][1]))\n",
        "    \n",
        "\n",
        "def sample_video_from_mp4(file, num_frames=16):\n",
        "    \"\"\"This function takes a mp4 video file as input and returns\n",
        "    a list of uniformly sampled frames (PIL Image).\n",
        "    Parameters\n",
        "    ----------\n",
        "    file : str\n",
        "        path to mp4 video file\n",
        "    num_frames : int\n",
        "        how many frames to select using uniform frame sampling.\n",
        "    Returns\n",
        "    -------\n",
        "    images: list of PIL Images\n",
        "    num_frames: int\n",
        "        number of frames extracted\n",
        "    \"\"\"\n",
        "    images = list()\n",
        "    vr = VideoReader(file, ctx=cpu(0))\n",
        "    #print(\"vr\", vr)\n",
        "    total_frames = len(vr)\n",
        "    #print(\"total frames\",total_frames)\n",
        "    indices = np.linspace(0,total_frames-1,num_frames,dtype=np.int)\n",
        "    for seg_ind in indices:\n",
        "        images.append(Image.fromarray(vr[seg_ind].asnumpy()))\n",
        "\n",
        "    return images,num_frames\n",
        "\n",
        "\n",
        "def get_activations_and_save(model, video_list, activations_dir):\n",
        "    #save path for extracted features\n",
        "    resize_normalize = trn.Compose([\n",
        "            trn.RandomResizedCrop(224),\n",
        "            trn.ToTensor(),\n",
        "            trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "    for video_file in tqdm(video_list):\n",
        "        vid,num_frames = sample_video_from_mp4(video_file)\n",
        "        video_file_name = os.path.split(video_file)[-1].split(\".\")[0]\n",
        "        activations = []\n",
        "        for frame,img in enumerate(vid):\n",
        "            input_img = V(resize_normalize(img).unsqueeze(0))\n",
        "            if torch.cuda.is_available():\n",
        "                input_img=input_img.cuda()\n",
        "\n",
        "            x = model.forward(input_img)\n",
        "            for i,feat in enumerate(x):\n",
        "                if frame==0:\n",
        "                    if i==7:\n",
        "                        print(\"\\nTop-5 Predictions for the video id: \", video_file_name)\n",
        "                        print_resnet_predictions(feat)\n",
        "                    activations.append(feat.data.cpu().numpy().ravel())\n",
        "                else:\n",
        "                    activations[i] =  activations[i] + feat.data.cpu().numpy().ravel()\n",
        "        \n",
        "        for layer in range(len(activations)):\n",
        "            save_path = os.path.join(activations_dir, video_file_name+\"_\"+\"layer\" + \"_\" + str(layer+1) + \".npy\")\n",
        "            avg_layer_activation = activations[layer]/float(num_frames)\n",
        "            np.save(save_path,avg_layer_activation)\n",
        "\n",
        "def do_PCA_and_save(activations_dir, save_dir):\n",
        "    layers = ['layer_1','layer_2','layer_3','layer_4','layer_5','layer_6','layer_7','layer_8']\n",
        "    n_components = 100\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    for layer in tqdm(layers):\n",
        "        activations_file_list = glob.glob(activations_dir +'/*'+layer+'.npy')\n",
        "        activations_file_list.sort()\n",
        "        feature_dim = np.load(activations_file_list[0])\n",
        "        x = np.zeros((len(activations_file_list),feature_dim.shape[0]))\n",
        "        for i,activation_file in enumerate(activations_file_list):\n",
        "            temp = np.load(activation_file)\n",
        "            x[i,:] = temp\n",
        "        x_train = x[:1000,:]\n",
        "        x_test = x[1000:,:]\n",
        "        start_time = time.time()\n",
        "        x_test = StandardScaler().fit_transform(x_test)\n",
        "        x_train = StandardScaler().fit_transform(x_train)\n",
        "        ipca = PCA(n_components=n_components,random_state=seed)\n",
        "        ipca.fit(x_train)\n",
        "        x_train = ipca.transform(x_train)\n",
        "        x_test = ipca.transform(x_test)\n",
        "        train_save_path = os.path.join(save_dir,\"train_\"+layer)\n",
        "        test_save_path = os.path.join(save_dir,\"test_\"+layer)\n",
        "        np.save(train_save_path,x_train)\n",
        "        np.save(test_save_path,x_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "423d1bbc",
      "metadata": {
        "id": "423d1bbc"
      },
      "outputs": [],
      "source": [
        "video_dir = '/Users/nam/Desktop/participants_data/AlgonautsVideos268_All_30fpsmax'\n",
        "video_list = glob.glob(video_dir + '/*.mp4')\n",
        "video_list.sort()\n",
        "print('Total Number of Videos: ', len(video_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66646ee2",
      "metadata": {
        "id": "66646ee2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "save_dir = \"/Users/nam/Desktop/activations_resnet\"\n",
        "checkpoint_path = '/Users/nam/Desktop/resnet.pth'\n",
        "url = \"https://download.pytorch.org/models/resnet50-19c8e357.pth\"\n",
        "urllib.request.urlretrieve(url, \"/Users/nam/Desktop/resnet.pth\")\n",
        "model = load_resnet(checkpoint_path)\n",
        "activations_dir = os.path.join(save_dir)\n",
        "if not os.path.exists(activations_dir):\n",
        "    os.makedirs(activations_dir)\n",
        "print(\"-------------Saving activations ----------------------------\")\n",
        "get_activations_and_save(model, video_list, activations_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a8a3bb",
      "metadata": {
        "id": "b7a8a3bb"
      },
      "outputs": [],
      "source": [
        "pca_dir = os.path.join(save_dir, 'pca_100')\n",
        "print(\"-------------perfnorming  PCA----------------------------\")\n",
        "do_PCA_and_save(activations_dir, pca_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86684e30",
      "metadata": {
        "id": "86684e30"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def vectorized_correlation(x,y):\n",
        "    dim = 0\n",
        "    centered_x = x - x.mean(axis=dim, keepdims=True)\n",
        "    centered_y = y - y.mean(axis=dim, keepdims=True)\n",
        "    covariance = (centered_x * centered_y).sum(axis=dim, keepdims=True)\n",
        "    bessel_corrected_covariance = covariance / (x.shape[dim] - 1)\n",
        "    x_std = x.std(axis=dim, keepdims=True)+1e-8\n",
        "    y_std = y.std(axis=dim, keepdims=True)+1e-8\n",
        "    corr = bessel_corrected_covariance / (x_std * y_std)\n",
        "    return corr.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246ec81d",
      "metadata": {
        "id": "246ec81d"
      },
      "outputs": [],
      "source": [
        "class OLS_pytorch(object):\n",
        "    def __init__(self,use_gpu=False):\n",
        "        self.coefficients = []\n",
        "        self.use_gpu = use_gpu\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        if len(X.shape) == 1:\n",
        "            X = self._reshape_x(X)\n",
        "        if len(y.shape) == 1:\n",
        "            y = self._reshape_x(y)\n",
        "        X =  self._concatenate_ones(X)\n",
        "        X = torch.from_numpy(X).float()\n",
        "        y = torch.from_numpy(y).float()\n",
        "        if self.use_gpu:\n",
        "            X = X.cuda()\n",
        "            y = y.cuda()\n",
        "        XtX = torch.matmul(X.t(),X)\n",
        "        Xty = torch.matmul(X.t(),y.unsqueeze(2))\n",
        "        XtX = XtX.unsqueeze(0)\n",
        "        XtX = torch.repeat_interleave(XtX, y.shape[0], dim=0)\n",
        "        betas_cholesky, _ = torch.solve(Xty, XtX)\n",
        "        self.coefficients = betas_cholesky\n",
        "\n",
        "    def predict(self, entry):\n",
        "        if len(entry.shape) == 1:\n",
        "            entry = self._reshape_x(entry)\n",
        "        entry =  self._concatenate_ones(entry)\n",
        "        entry = torch.from_numpy(entry).float()\n",
        "        if self.use_gpu:\n",
        "            entry = entry.cuda()\n",
        "        prediction = torch.matmul(entry,self.coefficients)\n",
        "        prediction = prediction.cpu().numpy()\n",
        "        prediction = np.squeeze(prediction).T\n",
        "        return prediction\n",
        "\n",
        "    def _reshape_x(self,X):\n",
        "        return X.reshape(-1,1)\n",
        "\n",
        "    def _concatenate_ones(self,X):\n",
        "        ones = np.ones(shape=X.shape[0]).reshape(-1,1)\n",
        "        return np.concatenate((ones,X),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d27bb6",
      "metadata": {
        "id": "03d27bb6"
      },
      "outputs": [],
      "source": [
        "def predict_fmri_fast(train_activations, test_activations, train_fmri,use_gpu=False):\n",
        "    #This function fits the regressor using train_activations and train_fmri\n",
        "    #then returns the predicted fmri_pred_test using the fitted weights and test_activations\n",
        "    reg = OLS_pytorch(use_gpu)\n",
        "    reg.fit(train_activations,train_fmri.T)\n",
        "    fmri_pred_test = reg.predict(test_activations)\n",
        "    return fmri_pred_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e26eb89",
      "metadata": {
        "id": "4e26eb89"
      },
      "outputs": [],
      "source": [
        "def perform_encoding(activation_dir, fmri_dir,results_dir, sub, layer, ROI = 'WB', mode = 'val', visualize_results = True\\, batch_size=1000):\n",
        "  if torch.cuda.is_available():\n",
        "      use_gpu = True\n",
        "  else:\n",
        "      use_gpu = False\n",
        "\n",
        "  activations_dir = '/Users/nam/Desktop/activations_resnet'\n",
        "  pca_dir = os.path.join(activations_dir,'pca_100')\n",
        "  train_activations,test_activations = get_activations(pca_dir, layer)\n",
        "\n",
        "  if ROI == \"WB\":\n",
        "      track = \"full_track\"\n",
        "  else:\n",
        "      track = \"mini_track\"\n",
        "  fmri_dir = os.path.join(fmri_dir, track)\n",
        "  sub_fmri_dir = os.path.join(fmri_dir, sub)\n",
        "  if track == \"full_track\":\n",
        "      fmri_train_all,voxel_mask = get_fmri(sub_fmri_dir,ROI)\n",
        "  else:\n",
        "      fmri_train_all = get_fmri(sub_fmri_dir,ROI)\n",
        "  num_voxels = fmri_train_all.shape[1]\n",
        "\n",
        "  if mode == 'val':\n",
        "      # for cv\n",
        "      test_activations = train_activations[900:,:]\n",
        "      train_activations = train_activations[:900,:]\n",
        "      fmri_train = fmri_train_all[:900,:]\n",
        "      fmri_test = fmri_train_all[900:,:]\n",
        "      pred_fmri = np.zeros_like(fmri_test)\n",
        "      pred_fmri_save_path = os.path.join(results_dir, ROI + '_val.npy')\n",
        "  else:\n",
        "      fmri_train = fmri_train_all\n",
        "      num_test_videos = 102\n",
        "      pred_fmri = np.zeros((num_test_videos,num_voxels))\n",
        "      pred_fmri_save_path = os.path.join(results_dir, ROI + '_test.npy')\n",
        "  \n",
        "  iter = 0\n",
        "  \n",
        "  while iter < num_voxels-batch_size:\n",
        "      pred_fmri[:,iter:iter+batch_size] = predict_fmri_fast(train_activations,test_activations,fmri_train[:,iter:iter+batch_size], use_gpu = use_gpu)\n",
        "      iter = iter+batch_size\n",
        "  pred_fmri[:,iter:] = predict_fmri_fast(train_activations,test_activations,fmri_train[:,iter:iter+batch_size], use_gpu = use_gpu)\n",
        "  if mode == 'val':\n",
        "    score = vectorized_correlation(fmri_test,pred_fmri)\n",
        "    print(\"----------------------------------------------------------------------------\")\n",
        "    print(\"Mean correlation for ROI : \",ROI, \"in \",sub,\" using \",layer, \" is :\", round(score.mean(), 6))\n",
        "\n",
        "\n",
        "  nii_save_path =  os.path.join(results_dir, ROI + '_val.nii')\n",
        "  ######## Result visualization ################\n",
        "  if track == \"full_track\" and visualize_results:\n",
        "    visual_mask_3D = np.zeros((78,93,71))\n",
        "    visual_mask_3D[voxel_mask==1]= score\n",
        "  brain_mask = '/Users/nam/Desktop/example.nii'\n",
        "  \n",
        "  saveasnii(brain_mask,nii_save_path,visual_mask_3D)\n",
        "  plotting.plot_glass_brain(nii_save_path,plot_abs=True,\n",
        "                      title='Correlation for ' + sub+ ' and ' + layer,\n",
        "                      display_mode='lyr',colorbar=True)\n",
        "  view = plotting.view_img_on_surf(nii_save_path, threshold=None, surf_mesh='fsaverage', title='Correlation for sub' + sub, colorbar=True)\n",
        "  view_save_path = os.path.join(results_dir, ROI + '_val.html')\n",
        "  view.save_as_html(view_save_path)\n",
        "  print(\"Results saved in this directory: \", results_dir)\n",
        "  view.open_in_browser()\n",
        "\n",
        "  np.save(pred_fmri_save_path, pred_fmri)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce6b795",
      "metadata": {
        "id": "8ce6b795"
      },
      "outputs": [],
      "source": [
        "sub = 'sub04'  #@param [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]  \n",
        "ROI = 'WB'  #@param [\"WB\", \"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n",
        "layer = 'layer_1' #@param [\"layer_1\",\"layer_2\",\"layer_3\",\"layer_4\",\"layer_5\",\"layer_6\",\"layer_7\",\"layer_8\"]\n",
        "fmri_dir = '/Users/nam/Desktop/participants_data/participants_data_v2021'\n",
        "prediction_dir = '/Users/nam/Desktop/prediction'\n",
        "model = 'resnet'\n",
        "if ROI == \"WB\":\n",
        "    track = \"full_track\"\n",
        "else:\n",
        "    track = \"mini_track\"\n",
        "\n",
        "results_dir = os.path.join(prediction_dir,model, layer, track, sub)\n",
        "if not os.path.exists(results_dir):\n",
        "  os.makedirs(results_dir)\n",
        "perform_encoding(activations_dir, fmri_dir, results_dir, sub, layer, ROI=ROI)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d74e34",
      "metadata": {
        "id": "c3d74e34"
      },
      "outputs": [],
      "source": [
        "subs = [\"sub01\",\"sub02\",\"sub03\",\"sub04\",\"sub05\",\"sub06\",\"sub07\",\"sub08\",\"sub09\",\"sub10\"]  \n",
        "ROIs = [\"WB\", \"V1\", \"V2\",\"V3\", \"V4\", \"LOC\", \"EBA\", \"FFA\",\"STS\", \"PPA\"]\n",
        "layer = 'layer_5'\n",
        "model = 'resalex'\n",
        "for sub in subs:\n",
        "  for ROI in ROIs:\n",
        "    if ROI == \"WB\":\n",
        "        track = \"full_track\"\n",
        "    else:\n",
        "        track = \"mini_track\"\n",
        "    results_dir = os.path.join(prediction_dir,model, layer,track, sub)\n",
        "    if not os.path.exists(results_dir):\n",
        "      os.makedirs(results_dir)\n",
        "    print (\"Starting ROI: \", ROI, \"sub: \",sub)\n",
        "    perform_encoding(activations_dir, fmri_dir,results_dir, sub, layer, ROI=ROI,mode='test')\n",
        "    print (\"Completed ROI: \", ROI, \"sub: \",sub)\n",
        "    print(\"----------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4d06c45",
      "metadata": {
        "id": "f4d06c45"
      },
      "outputs": [],
      "source": [
        "def prepare_results(down_dir, track):\n",
        "    if track == 'full_track':\n",
        "        ROIs = ['WB']\n",
        "    else:\n",
        "        ROIs = ['LOC','FFA','STS','EBA','PPA','V1','V2','V3','V4']\n",
        "    num_subs = 10\n",
        "    subs=[]\n",
        "    for s in range(num_subs):\n",
        "        subs.append('sub'+str(s+1).zfill(2))\n",
        "    results = {}\n",
        "    for ROI in ROIs:\n",
        "        ROI_results = {}\n",
        "        for sub in subs:\n",
        "            ROI_result_file = os.path.join(down_dir,track,sub,ROI+\"_test.npy\")\n",
        "            if not os.path.exists(ROI_result_file):\n",
        "                print(\"Result not found for \",sub, \" and ROI: \",ROI)\n",
        "                print(\"Result file path: \", ROI_result_file)\n",
        "                print(\"Please check if the directory is correct or generate predicted data for ROI: \",ROI , \" in subject: \", sub)\n",
        "                return\n",
        "            ROI_result = np.load(ROI_result_file)\n",
        "            ROI_results[sub] = ROI_result\n",
        "        results[ROI] = ROI_results\n",
        "\n",
        "    save_dict(results,track+\".pkl\")\n",
        "    zipped_results = zipfile.ZipFile(track+\".zip\", 'w')\n",
        "    zipped_results.write(track+\".pkl\")\n",
        "    zipped_results.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740ebaec",
      "metadata": {
        "id": "740ebaec"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "down_dir = '/Users/nam/Desktop/prediction/resalex/layer_5'\n",
        "prepare_results(down_dir, 'full_track')\n",
        "prepare_results(down_dir, 'mini_track')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e6b776",
      "metadata": {
        "id": "23e6b776"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Soft_Attention_with_Resnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}